<!doctype html>
<html lang="ru" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Neural-networks/Transformer/Transformer" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">Трансформер | Машинное и глубокое обучение</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://deepmachinelearning.ru/img/social-card2.png"><meta data-rh="true" name="twitter:image" content="https://deepmachinelearning.ru/img/social-card2.png"><meta data-rh="true" property="og:url" content="https://deepmachinelearning.ru/docs/Neural-networks/Transformer/Transformer"><meta data-rh="true" property="og:locale" content="ru"><meta data-rh="true" name="docusaurus_locale" content="ru"><meta data-rh="true" name="docsearch:language" content="ru"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Трансформер | Машинное и глубокое обучение"><meta data-rh="true" name="description" content="Модель трансформера - архитектура, преимущества, принцип работы."><meta data-rh="true" property="og:description" content="Модель трансформера - архитектура, преимущества, принцип работы."><meta data-rh="true" name="keywords" content="трансформеры модели,attention is all you need,transformer network,net transformer,modeling transformer,transformer model,трансформеры языковые модели,attention network,attention neural networks,тип нейронной сети,искусственные нейронные сети"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://deepmachinelearning.ru/docs/Neural-networks/Transformer/Transformer"><link data-rh="true" rel="alternate" href="https://deepmachinelearning.ru/docs/Neural-networks/Transformer/Transformer" hreflang="ru"><link data-rh="true" rel="alternate" href="https://deepmachinelearning.ru/docs/Neural-networks/Transformer/Transformer" hreflang="x-default"><link rel="preconnect" href="https://mc.yandex.ru">
<script>!function(e,t,a,c,n,r,i){e[n]=e[n]||function(){(e[n].a=e[n].a||[]).push(arguments)},e[n].l=1*new Date,r=t.createElement(a),i=t.getElementsByTagName(a)[0],r.async=1,r.src="https://mc.yandex.ru/metrika/tag.js",i.parentNode.insertBefore(r,i)}(window,document,"script",0,"ym"),ym(98444042,"init",{defer:!0,clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!1,ecommerce:!1,trackHash:!1})</script>
<noscript>
                            <div><img src="https://mc.yandex.ru/watch/98444042" style="position:absolute; left:-9999px;" alt=""></div>
                        </noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.8f598318.css">
<script src="/assets/js/runtime~main.bdcafe3d.js" defer="defer"></script>
<script src="/assets/js/main.832a0886.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Перейти к основному содержимому"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Перейти к основному содержимому</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Переключить навигационную панель" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.png" alt="" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/docs/Machine-learning/book-title">Машинное обучение</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/Neural-networks/book-title">Глубокое обучение</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/abbreviations">Обозначения</a><a class="navbar__item navbar__link" href="/license">Лицензия</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Переключение между темным и светлым режимом (сейчас используется Светлый режим)" aria-label="Переключение между темным и светлым режимом (сейчас используется Светлый режим)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Прокрутка к началу" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Neural-networks/book-title">Глубокое обучение</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Neural-networks/intro">Введение</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Tasks-of-deep-learning">Примеры задач глубокого обучения</a><button aria-label="Expand sidebar category &#x27;Примеры задач глубокого обучения&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Multilayer-perceptron">Основы нейросетевых архитектур</a><button aria-label="Expand sidebar category &#x27;Основы нейросетевых архитектур&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Training-neural-networks">Обучение нейросетей</a><button aria-label="Expand sidebar category &#x27;Обучение нейросетей&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Training-simplification">Упрощение настройки</a><button aria-label="Expand sidebar category &#x27;Упрощение настройки&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Regularization">Регуляризация при настройке нейросетей</a><button aria-label="Expand sidebar category &#x27;Регуляризация при настройке нейросетей&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Special-architectures">Специальные архитектуры</a><button aria-label="Expand sidebar category &#x27;Специальные архитектуры&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Convolution-for-sequences-1D">Локальная обработка последовательностей</a><button aria-label="Expand sidebar category &#x27;Локальная обработка последовательностей&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Convolution-for-images-2D">Локальная обработка изображений</a><button aria-label="Expand sidebar category &#x27;Локальная обработка изображений&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Convolutional-architectures">Основные свёрточные архитектуры</a><button aria-label="Expand sidebar category &#x27;Основные свёрточные архитектуры&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Semantic-segmentation">Семантическая сегментация</a><button aria-label="Expand sidebar category &#x27;Семантическая сегментация&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Object-detection">Детекция объектов</a><button aria-label="Expand sidebar category &#x27;Детекция объектов&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Instance-segmentation">Сегментация объектов</a><button aria-label="Expand sidebar category &#x27;Сегментация объектов&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Embeddings">Эмбеддинги слов и параграфов</a><button aria-label="Expand sidebar category &#x27;Эмбеддинги слов и параграфов&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Recurrent-neural-nets">Рекуррентные сети</a><button aria-label="Expand sidebar category &#x27;Рекуррентные сети&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/docs/Neural-networks/Transformer">Механизм внимания и трансформер</a><button aria-label="Collapse sidebar category &#x27;Механизм внимания и трансформер&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Neural-networks/Transformer/Transformer">Трансформер</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Neural-networks/Transformer/Encoder-self-attention">Кодировщик</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Neural-networks/Transformer/Positional-encoding">Позиционное кодирование</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Neural-networks/Transformer/Decoder">Декодировщик</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Neural-networks/Transformer/Training-transformer">Обучение трансформера</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Neural-networks/Graph-processing">Обработка графов</a><button aria-label="Expand sidebar category &#x27;Обработка графов&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Neural-networks/conclusion">Заключение</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Навигационная цепочка текущей страницы"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Главная страница" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/Neural-networks/Transformer"><span itemprop="name">Механизм внимания и трансформер</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Трансформер</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">Содержание этой страницы</button></div><div class="theme-doc-markdown markdown"><h1>Трансформер</h1>
<p><strong>Трансформер</strong> (transformer) - это нейросетевая модель, призванная более эффективно решать задачи <a href="/docs/Neural-networks/Recurrent-neural-nets/Application-Regimes#many-to-many">many-to-many</a> (seq2seq) по преобразованию одной последовательности в другую. Последовательностями могут выступать</p>
<ul>
<li>
<p>текст как последовательность слов,</p>
</li>
<li>
<p>видео как последовательности кадров,</p>
</li>
<li>
<p>звук как последовательность частот звуковых волн.</p>
</li>
</ul>
<p>Примерами таких задач могут быть</p>
<ul>
<li>
<p>система ответов на вопросы (вопрос-входная последовательность, ответ-выходная)</p>
</li>
<li>
<p>распознавание речи (входная последовательность - звук в виде спектрограммы, выходная - распознанная речь в виде текста)</p>
</li>
<li>
<p>прогнозирование временных рядов (входная последовательность-начало ряда, выходная - его продолжение)</p>
</li>
<li>
<p>музыкальная композиция (входная последовательност ь-текст песни, выходная-последовательность ноты).</p>
</li>
</ul>
<blockquote>
<p>В частности, модель ChatGPT расшифровывается как Chat Generative Pretrained Transformer и построена на базе трансформерной архитектуры.</p>
</blockquote>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>За пределами many-to-many</div><div class="admonitionContent_BuS1"><p>Отдельные элементы трансформера, такие как блок <strong>самовнимания</strong> (self-attention), используются и в других постановках, таких как one-to-one (классификация, генерация изображений), one-to-many (описание текстом, что показано на изображении), many-to-one (классификация текста), что позволяет говорить о других видах трансформера.</p><p>Например, использование самовнимания на изображениях называется <strong>визуальным трансформером</strong> (visual transformer). Поскольку трансформер работает над последовательностями, изображения в нём ра збиваются на последовательность фрагментов, после чего идёт работа с каждым фрагментом как с элементом &quot;последовательности&quot;.</p><p>Здесь же мы рассмотрим обработку классических последовательностей, заданных в явном виде, например, последовательность слов в тексте.</p></div></div>
<p>Далее в качестве many-to-many задачи будем рассматривать задачу <strong>машинного перевода</strong> (machine translation), состоящую в автоматическом переводе текста с одного языка на другой. Тем более, именно для этой задачи трансформер и был впервые предложен.</p>
<p>Ранее мы уже изучили, что для лучшей запоминаемости входной последовательности лучше использовать не <a href="/docs/Neural-networks/Recurrent-neural-nets/Application-Regimes#many-to-many">стандартную many-to-many архитектуру</a>, а дополненную <a href="/docs/Neural-networks/Recurrent-neural-nets/Attention-RNN">механизмом внимания</a> (attention). Но там на этапе генерации всё ещё используется обученная рекуррентая сеть, которой приходится запоминать весь уже сгенерированный контекст в скрытом состоянии. Поскольку оно представляется вектором фиксированного размера, то это всё ещё приводит к потере информации уже сгенерированного контекста. Эту проблему можно решить <u>повторным применением механизма внимания к уже сгенерированным токенам</u>, что и предложено в модели <strong>трансформера</strong> (transformer) [<a href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener noreferrer">1</a>], в котором полностью отказались от рекуррентности в кодировщике и декодировщике, заменим работу с историей блоками внимания.</p>
<blockquote>
<p>Собственно, поэтому статья [<a href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener noreferrer">1</a>] и называется - Attention is all you need, и является одной из самых цитируемых работ глубокого обучения, поскольку она принципиально изменила подход к обработке последовательностей.</p>
</blockquote>
<p>Предложенная модель трансформера сумела существенно повысить качество машинного перевода и стала повсеместно применяться для обработки и генерации последовательностей, вытесняя рекуррентные сети во всех случаях, кроме обработки коротких последовательностей и ситуаций, когда модель должна использовать мало памяти (трансформер в этом смысле более требователен).</p>
<blockquote>
<p>Поскольку трансформер основан исключительно на легко параллелизуемом механизме внимания безо всякого использования рекуррентности, это позволяет <u>быстрее его обучать</u> на видеокарте <u>с достаточным объёмом памяти</u>.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="архитектура-трансформера">Архитектура трансформера<a class="hash-link" aria-label="Прямая ссылка на Архитектура трансформера" title="Прямая ссылка на Архитектура трансформера" href="/docs/Neural-networks/Transformer/Transformer#архитектура-трансформера">​</a></h2>
<p>Схема модели трансформера приведена ниже [<a href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener noreferrer">1</a>]:</p>
<p><img decoding="async" loading="lazy" src="/assets/images/transformer-0df39d77f5ff83503b1b5db95c0bc0f3.png" width="692" height="998" class="img_ev3q"></p>
<p>Трансформер состоит из 2-х блоков - <strong>кодировщика</strong> (encoder), расположенного на схеме слева, и <strong>декодировщика</strong> (decoder), расположенного справа. Кодировщик и декодировщик выделены прозрачными прямоугольниками.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="применение-трансформера">Применение трансформера<a class="hash-link" aria-label="Прямая ссылка на Применение трансформера" title="Прямая ссылка на Применение трансформера" href="/docs/Neural-networks/Transformer/Transformer#применение-трансформера">​</a></h2>
<p>Обработка последовательностей, как всегда в нейросетях, происходит <a href="/docs/Neural-networks/Training/Opt-methods-fixed-lr#%D1%81%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9-%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9-%D1%81%D0%BF%D1%83%D1%81%D0%BA-%D0%BF%D0%BE-%D0%BC%D0%B8%D0%BD%D0%B8-%D0%B1%D0%B0%D1%82%D1%87%D0%B0%D0%BC">минибатчами</a>. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span> - максимальная длина входной последовательности в минибатче последовательностей длин <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>T</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>T</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">T_1,T_2,...T_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>T</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">{</mo><msub><mi>T</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>T</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>T</mi><mi>B</mi></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">T=\max\{T_1,T_2,...T_B\}.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">max</span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">}</span><span class="mord">.</span></span></span></span></span>
<p>Поскольку последовательности будут иметь разную длину, то каждая из них дополняется специальным токеном [EOS] (end of sequence), означающим конец последовательности, а более короткие последовательности после [EOS] дополняются спец. токеном [PAD], чтобы все они в итоге оказались одной длины для параллельной обработки.</p>
<p>Рассмотрим пример машинного перевода с русского на английский. Минибатч, состоящий из 3-х предложений на русском</p>
<ul>
<li>
<p>[Город располагается на берегу реки]</p>
</li>
<li>
<p>[В центре находится парк]</p>
</li>
<li>
<p>[Популярное место]</p>
</li>
</ul>
<p>будет представлен как</p>
<ul>
<li>
<p>[Город располагается на берегу реки EOS]</p>
</li>
<li>
<p>[В центре находится парк EOS PAD]</p>
</li>
<li>
<p>[Популярное место EOS PAD PAD PAD]</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="кодировщик">Кодировщик<a class="hash-link" aria-label="Прямая ссылка  на Кодировщик" title="Прямая ссылка на Кодировщик" href="/docs/Neural-networks/Transformer/Transformer#кодировщик">​</a></h2>
<p>На вход кодировщику поступает входная последовательность из <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span> токенов (слов), каждый из которых кодируется <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span>-мерным эмбеддингом (input embedding), <u>обучаемым вместе с настройкой самой сети</u> (хотя при желании их можно инициализировать и <a href="/docs/Neural-networks/Embeddings/Word2vec">готовыми эмбеддингами</a>).</p>
<blockquote>
<p>Поскольку в кодировщике полностью отказались от рекуррентности, он может параллельно обрабатывать всю входную последовательность, что существенно ускоряет его работу.</p>
</blockquote>
<p>Но информация о расположениях токенов внутри последовательности теряется. Чтобы модели в явном виде сообщить, какие токены где располагались, к эмбеддингу каждого токена на входе в декодировщик прибавляется эмбеддинг той же размерности, кодирующий абсолютное расположение эмбеддинга Это называется <a href="/docs/Neural-networks/Transformer/Positional-encoding">позиционным кодированием</a> (positional encoding).</p>
<p>Выходом кодировщика является <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span> эмбеддингов входных элементов последовательности, <u>уточнённых с учётом контекста всей последовательности</u>.</p>
<blockquote>
<p>Например, для 2-х последовательностей:</p>
<ul>
<li>
<p>замок имеет шесть башен и окружён глубоким рвом</p>
</li>
<li>
<p>замок заржавел, и ключ в нём не поворачивается</p>
</li>
</ul>
<p>эмбеддинг слова &quot;замок&quot; будет разным исходя из контекста!</p>
</blockquote>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Блоки кодировщика</div><div class="admonitionContent_BuS1"><p>Кодировщик состоит из <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span> последовательно применяемых <a href="/docs/Neural-networks/Transformer/Encoder-self-attention">блоков кодировщика</a>, каждый - со своими параметрами. В [<a href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener noreferrer">1</a>] бралось <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">N=6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">6</span></span></span></span>.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="декодировщик">Декодировщик<a class="hash-link" aria-label="Прямая ссылка на Декодировщик" title="Прямая ссылка на Декодировщик" href="/docs/Neural-networks/Transformer/Transformer#декодировщик">​</a></h2>
<p>Декодировщик работает аналогичным образом, только ему подаются эмбеддинги не входных токенов (слова на русском), а выходных (слова на английском, при переводе с русского на английский).</p>
<p>В режиме обучения - это слова корректного перевода (режим <a href="/docs/Neural-networks/Recurrent-neural-nets/Sequence-generation#%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%BE%D0%B9-%D1%81%D0%B5%D1%82%D0%B8">teacher forcing</a>), а в режиме применения - это слова, которые трансформер сам сгенерировал ранее.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="обучение-декодировщика">Обучение декодировщика<a class="hash-link" aria-label="Прямая ссылка на Обучение декодировщика" title="Прямая ссылка на Обучение декодировщика" href="/docs/Neural-networks/Transformer/Transformer#обучение-декодировщика">​</a></h3>
<p>Во время обучения декодировщику подается на вход <u>выходная</u> последовательность (корректный перевод на английском), сдвинутая на единицу специальным токеном [BOS] (beginning of sequence) и законченная токеном [EOS] (end of sequence) c дополнением PAD для выравнивания по максимальной длине выходной последовательнос ти в минибатче.</p>
<p>Например, обучающий минибатч ответов</p>
<ul>
<li>
<p>[The city is located on the river bank]</p>
</li>
<li>
<p>[There is a park in the center]</p>
</li>
<li>
<p>[A popular place]</p>
</li>
</ul>
<p>будет закодирован как</p>
<ul>
<li>
<p>[BOS The city is located on the river bank EOS]</p>
</li>
<li>
<p>[BOS There is a park in the center EOS PAD]</p>
</li>
<li>
<p>[BOS A popular place EOS PAD PAD PAD PAD PAD]</p>
</li>
</ul>
<p>для выравнивания длин всех последовательностей.</p>
<p>На выходе кодировщик выдаёт эмбеддинг для каждого токена перевода, к каждому из которых в отдельности применяется линейный слой (Linear на схеме) и <a href="/docs/Neural-networks/MLP/Outputs-loss-functions#softmax-%D0%BF%D1%80%D0%B5%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5">SoftMax преобразование</a>, выдающее вероятностное распределение на возможных словах перевода, по которым можно предсказать уже само слово, выбрав самое вероятное.</p>
<p>Таким образом, по входу</p>
<p>[BOS A popular place EOS PAD PAD PAD PAD PAD]</p>
<p>ожидается выход</p>
<p>[A popular place EOS X X X X X X]</p>
<p>где по позициям X потери уже не считаются, поскольку всё, что следует после токена [EOS], не будет участвовать в выходе сети.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Параллелизация</div><div class="admonitionContent_BuS1"><p>Поскольку ни кодировщик, ни декодировщик не используют рекуррентности, то их можно обучать синхронно на <u>всей последовательности целиком</u>. Это свойство позволило обучать трансформер гораздо быстрее, чем рекуррентные сети, решающие ту же задачу.</p></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="применение-декодировщика">Применение декодировщика<a class="hash-link" aria-label="Прямая ссылка на Применение декодировщика" title="Прямая ссылка на Применение декодировщика" href="/docs/Neural-networks/Transformer/Transformer#применение-декодировщика">​</a></h3>
<p>Во время применения <u>кодировщик</u> всё так же может обрабатывать входную последовательность параллельно, поскольку она известна целиком.</p>
<p>А вот выходная последовательность заранее неизвестна, поэтому <u>декодировщик</u> запускается много раз в авторегрессионном режиме (генерируя слова перевода последовательно одно за другим).</p>
<p>Например, при переводе [По пулярное место EOS]:</p>
<ol>
<li>
<p>подаётся [BOS], он генерирует [A].</p>
</li>
<li>
<p>подаётся [BOS A], он генерирует [A popular].</p>
</li>
<li>
<p>подаётся [BOS A popular], он генерирует [A popular place].</p>
</li>
<li>
<p>подаётся [BOS A popular place], он генерирует [A popular place EOS].</p>
</li>
</ol>
<p>После получения токена [EOS] генерация останавливается.</p>
<p>На каждом запуске декодировщик может предсказывать распределение вероятностей на всех уже сгенерированных токенах, но нас интересует только последнее сгенерированное слово, чтобы дополнить им выходную последовательность.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Блоки декодировщика</div><div class="admonitionContent_BuS1"><p>Декодировщик состоит из <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span> последовательно применяемых <a href="/docs/Neural-networks/Transformer/Decoder">блоков декодировщика</a>, каждый - со своими параметрами. В [<a href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener noreferrer">1</a>] бралось <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">N=6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">6</span></span></span></span>.</p></div></div>
<hr>
<p>Далее будет детально рассмотрена архитектура отдельных блоков <a href="/docs/Neural-networks/Transformer/Encoder-self-attention">кодировщика</a> и <a href="/docs/Neural-networks/Transformer/Decoder">декодировщика</a>. Мы изучим, как дополнить каждый входной токен информацией о его расположении во входной последовательности, используя <a href="/docs/Neural-networks/Transformer/Positional-encoding">позиционное кодирование</a>, а также разберём технические детали <a href="/docs/Neural-networks/Transformer/Training-transformer">обучения трансформера</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="литература">Литература<a class="hash-link" aria-label="Прямая ссылка на Литература" title="Прямая ссылка на Литература" href="/docs/Neural-networks/Transformer/Transformer#литература">​</a></h2>
<ol>
<li><a href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener noreferrer">Vaswani A. Attention is all you need //Advances in Neural Information Processing Systems. – 2017.</a></li>
<li><a href="https://medium.com/analytics-vidhya/attention-is-all-you-need-demystifying-the-transformer-revolution-in-nlp-68a2a5fbd95b" target="_blank" rel="noopener noreferrer">https://medium.com/analytics-vidhya/attention-is-all-you-need-demystifying-the-transformer-revolution-in-nlp-68a2a5fbd95b</a></li>
</ol></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Страница документа"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Neural-networks/Transformer"><div class="pagination-nav__sublabel">Предыдущая страница</div><div class="pagination-nav__label">Механизм внимания и трансформер</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Neural-networks/Transformer/Encoder-self-attention"><div class="pagination-nav__sublabel">Следующая страница</div><div class="pagination-nav__label">Кодировщик</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a class="table-of-contents__link toc-highlight" href="/docs/Neural-networks/Transformer/Transformer#архитектура-трансформера">Архитектура трансформера</a></li><li><a class="table-of-contents__link toc-highlight" href="/docs/Neural-networks/Transformer/Transformer#применени е-трансформера">Применение трансформера</a></li><li><a class="table-of-contents__link toc-highlight" href="/docs/Neural-networks/Transformer/Transformer#кодировщик">Кодировщик</a></li><li><a class="table-of-contents__link toc-highlight" href="/docs/Neural-networks/Transformer/Transformer#декодировщик">Декодировщик</a><ul><li><a class="table-of-contents__link toc-highlight" href="/docs/Neural-networks/Transformer/Transformer#обучение-декодировщика">Обучение декодировщика</a></li><li><a class="table-of-contents__link toc-highlight" href="/docs/Neural-networks/Transformer/Transformer#применение-декодировщика">Применение декодировщика</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/docs/Neural-networks/Transformer/Transformer#литература">Литература</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">© 2023-25 <a href="https://victorkitov.github.io/">Виктор Китов</a>.</div></div></div></footer></div>
</body>
</html>